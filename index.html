<h1 id="a-short-introduction-to-the-f-adjoint-notion"><strong>A short
introduction to the F-adjoint notion</strong></h1>
<p>This project will give some highlight on the notion of F-adjoint
which has been recently introduced in the following arxiv preprint: “<a
href="https://arxiv.org/abs/2304.13820">Backpropagation and F-adjoint.
arXiv preprint arXiv:2304.13820.</a>”. For this purpose, we consider the
simple case of a fully-connected deep multi-layer perceptron (MLP)
composed of <span class="math inline"><em>L</em></span> layers trained
in a supervised setting. We will denote such an architecture by <span
class="math inline">[mathjaxinline] A[N_0, \cdots, N_\ell,\cdots, N_L][/mathjaxinline]</span> where
<span class="math inline"><em>N</em><sub>0</sub></span> is the size of
the input layer, <span class="math inline"><em>N</em><sub>ℓ</sub></span>
is the size of hidden layer <span class="math inline">ℓ</span>, and
<span class="math inline"><em>N</em><sub><em>L</em></sub></span> is the
size of the output layer; <span class="math inline"><em>L</em></span> is
defined as the depth of the (MLP).</p>
<p><strong>Notations</strong></p>
<pre class="math"><code> [mathjax]  \begin{array}{|l|l|}
  \hline\\
  Term &amp; Description \\ \hline
 W^{\ell}\in\mathbb{R}^{N_{\ell}\times N_{\ell-1}} &amp; \mathrm{Weight\ matrix\ of\ the\ layer}\ \ell\  \mathrm{with\ bias} \\ \hline
 W_\sharp^{\ell}\in\mathbb{R}^{N_{\ell}\times (N_{\ell-1}-1)} &amp; \mathrm{Weight\ matrix\ of\ the\ layer}\ \ell\  \mathrm{without\ bias}  \\ \hline
 Y^{\ell}\in\mathbb{R}^{N_{\ell}}  &amp; \mathrm{Preactivation\ vector\ at\ layer}\ {\ell}, Y^{\ell} = W^{\ell}X^{\ell-1} \\ \hline
 X^{\ell}\in\mathbb{R}^{(N_{\ell}-1)}\times\{1\} &amp; \mathrm{Activition\ vector\ at\ the\ layer} \ {\ell} \  \mathrm{with\ bias}, X^{\ell} =\sigma^{\ell}(Y^{\ell})\\ \hline
 \sigma^\ell:\  \mathbb{R}^{N_{\ell}}\ni Y^{\ell}\longmapsto\sigma^{\ell}(Y^{\ell})\in\mathbb{R}^{N_{\ell}} &amp; \mathrm{Coordinate-wise\ activition\ function\ of\ the\ layer}\ {\ell}\\ \hline
   \end{array}[/mathjax]</code></pre>
<h1 id="definitions"><strong>Definitions</strong></h1>
<p>We shall recall and revise the definition of the this notion and
provide some straightforward properties and improvements of this
adjoint-like representation.</p>
<h2 id="definition-of-an-f-propagation">Definition of an <span
class="math inline"><em>F</em></span>-propagation</h2>
<p>Let <span
class="math inline"><em>X</em><sup>0</sup> ∈ ℝ<sup><em>N</em><sub>0</sub></sup></span>
be a given data, <span class="math inline"><em>σ</em></span> be a
coordinate-wise map from <span
class="math inline">ℝ<sup><em>N</em><sub>ℓ</sub></sup></span> into <span
class="math inline">ℝ<sup><em>N</em><sub>ℓ</sub></sup></span> and <span
class="math inline"><em>W</em><sup>ℓ</sup> ∈ ℝ<sup><em>N</em><sub>ℓ</sub> × <em>N</em><sub>ℓ − 1</sub></sup></span>
for all <span class="math inline">1 ≤ ℓ ≤ <em>L</em></span>. We say that
we have a two-step recursive F-propagation <span
class="math inline"><em>F</em></span> through the (MLP) <span
class="math inline"><em>A</em>[<em>N</em><sub>0</sub>,⋯,<em>N</em><sub><em>L</em></sub>]</span>
if one has the following family of vectors</p>
<pre class="math"><code>F(X^0):=\begin{Bmatrix}X^0, Y^{1},X^{1},\cdots,X^{L-1},Y^{L},X^{L}\end{Bmatrix}\  \mathrm{where}\  Y^\ell=W^{\ell}X^{\ell-1}, \ X^\ell=\sigma(Y^\ell),\ \ell=1,\cdots, L.</code></pre>
<p>Before going further, let us point that in the above definition the
prefix “F” stands for “Feed-forward”.</p>
<p>As a consequense, one may rewrite the <span
class="math inline"><em>F</em></span>-propagation algorithm as
follows:</p>
<ol type="1">
<li><p>Require: <span class="math inline">$`X^0,W,\sigma
`$</span></p></li>
<li><p>Ensure: F-propagation (<span class="math inline">$`
F(X^0)`$</span>)</p>
<p>Function: <span class="math inline"><em>F</em></span>-propagation
(<span class="math inline"><em>F</em></span>)</p>
<ol type="1">
<li><p><span class="math inline">$`F\leftarrow\{X^0\}`$</span></p></li>
<li><p>For <span class="math inline">ℓ = 1</span> to <span
class="math inline"><em>L</em></span>:</p>
<p><span
class="math inline"><em>Y</em><sup>ℓ</sup> := <em>W</em><sup>ℓ</sup><em>X</em><sup>ℓ − 1</sup></span></p>
<p><span
class="math inline"><em>X</em><sup>ℓ</sup> := <em>σ</em>(<em>Y</em><sup>ℓ</sup>)</span></p>
<p><span
class="math inline"><em>F</em> ← <em>Y</em><sup>ℓ</sup>, <em>X</em><sup>ℓ</sup></span></p>
<p>End For</p></li>
</ol></li>
</ol>
<p>Return <span class="math inline"><em>F</em></span></p>
<h2 id="definition-of-the-f-adjoint-of-an-f-propagation">Definition of
the <span class="math inline"><em>F</em></span>-adjoint of an <span
class="math inline"><em>F</em></span>-propagation</h2>
<p>Let <span class="math inline">$`X^0\in\mathbb{R}^{N_0}`$</span> be a
given data and let <span
class="math inline">$`X^L_*\in\mathbb{R}^{N_L}`$</span> be a given
vector. We define the F-adjoint propagation <span
class="math inline"><em>F</em><sub>*</sub></span>, through the (MLP)
<span class="math inline">$`A[N_0,\cdots, N_L]`$</span>, associated to
the F-propagation <span
class="math inline"><em>F</em>(<em>X</em><sup>0</sup>)</span> as
follows</p>
<pre class="math"><code>F_{*}(X^{0}, X^{L}_{*}):=\begin{Bmatrix} X^{L}_{*}, Y^{L}_{*}, X^{L-1}_{*},\cdots, X^{1}_{*},Y^{1}_{*}, X^{0}_{*} \end{Bmatrix}\  \mathrm{where}\  Y^\ell_{*}=X^{\ell}_{*}\odot {\sigma}&#39;(Y^\ell), \ X^{\ell-1}_{*}=(W_\sharp^\ell)^\top Y^\ell_{*},\ \ell=L,\cdots, 1.</code></pre>
<p>Also, one may write a similar algorithm for the <span
class="math inline"><em>F</em></span>-adjoint propagation as:</p>
<ol type="1">
<li><p>Require: <span class="math inline">$`F(X^0),X^L_*,W_\sharp,
\sigma' `$</span></p></li>
<li><p>Ensure: <span class="math inline">$`F_*`$</span>-propagation
(<span class="math inline">$`F_*(X^0, X_*^L)`$</span>)</p>
<p>Function: <span
class="math inline"><em>F</em><sub>*</sub></span>-propagation (<span
class="math inline">$`F_*`$</span>)</p>
<ol type="1">
<li><p><span class="math inline">$`F_* \leftarrow
\{X_*^L\}`$</span></p></li>
<li><p>For <span class="math inline">ℓ = <em>L</em></span> to <span
class="math inline">1</span>:</p>
<p><span
class="math inline"><em>Y</em><sub>*</sub><sup>ℓ</sup> := <em>X</em><sub>*</sub><sup>ℓ</sup> ⊙ <em>σ</em>′(<em>Y</em><sup>ℓ</sup>)</span></p>
<p>$X<sup>{}:={(W_</sup>)}<sup>Y</sup>_* $</p>
<p>$F_* Y^_* $, <span
class="math inline"><em>X</em><sub>*</sub><sup>ℓ − 1</sup></span></p>
<p>End For</p></li>
</ol></li>
</ol>
<p>Return <span class="math inline"><em>F</em><sub>*</sub></span></p>
<h2 id="reference">Reference</h2>
<div id="refs" class="references" role="doc-bibliography">
<p>Boughammoura, A. (2023). Backpropagation and F-adjoint. arXiv
preprint arXiv:2304.13820.(https://arxiv.org/abs/2304.13820)</p>
</div>
